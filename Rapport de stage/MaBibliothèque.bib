
@online{noauthor_snt_nodate,
	title = {{SNT} Photo Filtrage},
	url = {https://www.infoforall.fr/act/snt/filtrage-d-une-image/},
	urldate = {2025-05-27},
}

@online{noauthor_copier_nodate,
	title = {Copier un fichier en Python},
	url = {https://www.techiedelight.com/fr/copy-file-python/},
	urldate = {2025-05-27},
	file = {Copier un fichier en Python:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/SNR5TGXD/copy-file-python.html:text/html},
}

@online{ultralytics_configuration_nodate,
	title = {Configuration},
	url = {https://docs.ultralytics.com/usage/cfg},
	abstract = {Optimize your Ultralytics {YOLO} model's performance with the right settings and hyperparameters. Learn about training, validation, and prediction configurations.},
	author = {Ultralytics},
	urldate = {2025-05-23},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/49THX4KZ/cfg.html:text/html},
}

@online{noauthor_finally_2023,
	title = {Finally understand Anchor Boxes in Object Detection (2D and 3D)},
	url = {https://www.thinkautonomous.ai/blog/anchor-boxes/},
	abstract = {What are anchor boxes? How do they work? Why are they needed? It seems like the concept is harder and harder to get.
Yet, it can be very simple.},
	titleaddon = {Welcome to The Library!},
	urldate = {2025-05-23},
	date = {2023-05-02},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/27RZUD9I/anchor-boxes.html:text/html},
}

@online{noauthor_what_2020,
	title = {What are Anchor Boxes in Object Detection?},
	url = {https://blog.roboflow.com/what-is-an-anchor-box/},
	abstract = {Learn what anchor boxes are and why they are used in object detection.},
	titleaddon = {Roboflow Blog},
	urldate = {2025-05-23},
	date = {2020-07-13},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/77CAY3U6/what-is-an-anchor-box.html:text/html},
}

@online{noauthor_what_2020-1,
	title = {What are Anchor Boxes in Object Detection?},
	url = {https://blog.roboflow.com/what-is-an-anchor-box/},
	abstract = {Learn what anchor boxes are and why they are used in object detection.},
	titleaddon = {Roboflow Blog},
	urldate = {2025-05-23},
	date = {2020-07-13},
	langid = {english},
}

@online{sufficient_chain_431_how_2024,
	title = {How to do data augmentation on a {YOLO} annotated dataset?},
	url = {https://www.reddit.com/r/computervision/comments/1fvw0nn/how_to_do_data_augmentation_on_a_yolo_annotated/},
	titleaddon = {r/computervision},
	author = {Sufficient{\textbackslash}\_Chain{\textbackslash}\_431},
	urldate = {2025-05-21},
	date = {2024-10-04},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/ZDUJ35A7/how_to_do_data_augmentation_on_a_yolo_annotated.html:text/html},
}

@online{ultralytics_augmentation_nodate,
	title = {Augmentation des données {YOLO}},
	url = {https://docs.ultralytics.com/fr/guides/yolo-data-augmentation},
	abstract = {Apprenez les techniques essentielles d'augmentation des données dans Ultralytics {YOLO}. Explorer les différentes transformations, leurs impacts, et comment les mettre en œuvre efficacement pour améliorer les performances du modèle.},
	author = {Ultralytics},
	urldate = {2025-05-21},
	langid = {french},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/S7IFM5V9/yolo-data-augmentation.html:text/html},
}

@online{noauthor_ffmpeg_nodate,
	title = {{FFmpeg}},
	url = {https://ffmpeg.org/},
	urldate = {2025-06-03},
	file = {FFmpeg:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/M44CZIDV/ffmpeg.org.html:text/html},
}

@article{dormagen_machine_2023,
	title = {Machine learning reveals the waggle drift’s role in the honey bee dance communication system},
	volume = {2},
	issn = {2752-6542},
	url = {https://doi.org/10.1093/pnasnexus/pgad275},
	doi = {10.1093/pnasnexus/pgad275},
	abstract = {The honey bee waggle dance is one of the most prominent examples of abstract communication among animals: successful foragers convey new resource locations to interested followers via characteristic “dance” movements in the nest, where dances advertise different locations on different overlapping subregions of the “dance floor.” To this day, this spatial separation has not been described in detail, and it remains unknown how it affects the dance communication. Here, we evaluate long-term recordings of Apis mellifera foraging at natural and artificial food sites. Using machine learning, we detect and decode waggle dances, and we individually identify and track dancers and dance followers in the hive and at artificial feeders. We record more than a hundred thousand waggle phases, and thousands of dances and dance-following interactions to quantitatively describe the spatial separation of dances on the dance floor. We find that the separation of dancers increases throughout a dance and present a motion model based on a positional drift of the dancer between subsequent waggle phases that fits our observations. We show that this separation affects follower bees as well and results in them more likely following subsequent dances to similar food source locations, constituting a positive feedback loop. Our work provides evidence that the positional drift between subsequent waggle phases modulates the information that is available to dance followers, leading to an emergent optimization of the waggle dance communication system.},
	pages = {pgad275},
	number = {9},
	journaltitle = {{PNAS} Nexus},
	shortjournal = {{PNAS} Nexus},
	author = {Dormagen, David M and Wild, Benjamin and Wario, Fernando and Landgraf, Tim},
	urldate = {2025-06-03},
	date = {2023-09-01},
	file = {Full Text PDF:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/TX5XNCYS/Dormagen et al. - 2023 - Machine learning reveals the waggle drift’s role in the honey bee dance communication system.pdf:application/pdf},
}

@online{noauthor_opencv_nodate,
	title = {{OpenCV}: Adding (blending) two images using {OpenCV}},
	url = {https://docs.opencv.org/3.4/d5/dc4/tutorial_adding_images.html},
	urldate = {2025-06-03},
}

@online{noauthor_wpmcommonwagglephasepy_2024,
	title = {wpm/common/wagglephase.py · main · Sylvain Galopin / {WagglePhaseMapper} · {GitLab}},
	url = {https://gitlab.com/sgalopin/waggle_phase_mapper/-/blob/main/wpm/common/wagglephase.py?ref_type=heads},
	abstract = {{GitLab}.com},
	titleaddon = {{GitLab}},
	urldate = {2025-06-03},
	date = {2024-10-08},
	langid = {english},
}

@online{ultralytics_hyperparameter_nodate,
	title = {Hyperparameter Tuning},
	url = {https://docs.ultralytics.com/guides/hyperparameter-tuning},
	abstract = {Master hyperparameter tuning for Ultralytics {YOLO} to optimize model performance with our comprehensive guide. Elevate your machine learning models today!.},
	author = {Ultralytics},
	urldate = {2025-06-03},
	langid = {english},
}

@online{ultralytics_train_nodate,
	title = {Train},
	url = {https://docs.ultralytics.com/modes/train},
	abstract = {Learn how to efficiently train object detection models using {YOLO}11 with comprehensive instructions on settings, augmentation, and hardware utilization.},
	author = {Ultralytics},
	urldate = {2025-06-03},
	langid = {english},
}

@online{ultralytics_configuration_nodate-1,
	title = {Configuration},
	url = {https://docs.ultralytics.com/usage/cfg},
	abstract = {Optimize your Ultralytics {YOLO} model's performance with the right settings and hyperparameters. Learn about training, validation, and prediction configurations.},
	author = {Ultralytics},
	urldate = {2025-06-03},
	langid = {english},
}

@online{ultralytics_hyperparameter_nodate-1,
	title = {Hyperparameter Tuning},
	url = {https://docs.ultralytics.com/guides/hyperparameter-tuning},
	abstract = {Master hyperparameter tuning for Ultralytics {YOLO} to optimize model performance with our comprehensive guide. Elevate your machine learning models today!.},
	author = {Ultralytics},
	urldate = {2025-06-03},
	langid = {english},
}

@online{ultralytics_hyperparameter_nodate-2,
	title = {Hyperparameter Tuning},
	url = {https://docs.ultralytics.com/guides/hyperparameter-tuning},
	abstract = {Master hyperparameter tuning for Ultralytics {YOLO} to optimize model performance with our comprehensive guide. Elevate your machine learning models today!.},
	author = {Ultralytics},
	urldate = {2025-06-03},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/QWRQPXKC/hyperparameter-tuning.html:text/html},
}

@online{noauthor_ultralytics_nodate,
	title = {Ultralytics {YOLO} Hyperparameter Tuning Guide - Ultralytics {YOLO} Docs},
	url = {https://docs.ultralytics.com/guides/hyperparameter-tuning/#introduction},
	urldate = {2025-06-03},
	file = {Ultralytics YOLO Hyperparameter Tuning Guide - Ultralytics YOLO Docs:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/NGAC7KCI/hyperparameter-tuning.html:text/html},
}

@online{ultralytics_configuration_nodate-2,
	title = {Configuration},
	url = {https://docs.ultralytics.com/usage/cfg},
	abstract = {Optimize your Ultralytics {YOLO} model's performance with the right settings and hyperparameters. Learn about training, validation, and prediction configurations.},
	author = {Ultralytics},
	urldate = {2025-06-03},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/24753Y73/cfg.html:text/html},
}

@video{abonia_sojasingarayar_fine-tuning_2024,
	title = {Fine-Tuning {YOLOv}10 for Object Detection on a Custom Dataset \#yolo \#finetuning},
	url = {https://www.youtube.com/watch?v=QCXCbbi5w-k},
	author = {{Abonia Sojasingarayar}},
	urldate = {2025-06-03},
	date = {2024-07-05},
}

@online{noauthor_youtubecomwatchvqcxcbbi5w-k_nodate,
	title = {youtube.com/watch?v={QCXCbbi}5w-k},
	url = {https://www.youtube.com/watch?v=QCXCbbi5w-k},
	urldate = {2025-06-03},
	file = {youtube.com/watch?v=QCXCbbi5w-k:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/Y258PSVH/watch.html:text/html},
}

@online{noauthor_ultralytics_nodate-1,
	title = {Ultralytics {YOLO} Hyperparameter Tuning Guide - Ultralytics {YOLO} Docs},
	url = {https://docs.ultralytics.com/fr/guides/hyperparameter-tuning/},
	urldate = {2025-06-03},
}

@online{noauthor_apercu_nodate,
	title = {Aperçu de l'évaluation et de la mise au point des modèles - Ultralytics {YOLO} Docs},
	url = {https://docs.ultralytics.com/fr/guides/model-evaluation-insights/#confidence-score},
	urldate = {2025-06-03},
}

@online{noauthor_how_nodate,
	title = {How to Fine Tune {YOLOv}8: Step-by-Step Guide {\textbar} yolov8},
	url = {https://yolov8.org/how-to-use-fine-tune-yolov8/#Fine-Tuning_Steps_How_to_Use},
	urldate = {2025-06-03},
	file = {How to Fine Tune YOLOv8\: Step-by-Step Guide | yolov8:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/SMC2X3U8/how-to-use-fine-tune-yolov8.html:text/html},
}

@online{noauthor_understanding_nodate,
	title = {Understanding Evaluation Metrics in Medical Image Segmentation {\textbar} by Nghi Huynh {\textbar} Medium},
	url = {https://medium.com/@nghihuynh_37300/understanding-evaluation-metrics-in-medical-image-segmentation-d289a373a3f},
	urldate = {2025-06-03},
}

@online{noauthor_deep_nodate,
	title = {A Deep Dive Into Semantic Segmentation Evaluation Metrics {\textbar} {HackerNoon}},
	url = {https://hackernoon.com/a-deep-dive-into-semantic-segmentation-evaluation-metrics},
	urldate = {2025-06-03},
}

@online{noauthor_what_nodate,
	title = {What are different evaluation metrics used to evaluate image segmentation models? {\textbar} {GeeksforGeeks}},
	url = {https://www.geeksforgeeks.org/what-are-different-evaluation-metrics-used-to-evaluate-image-segmentation-models/},
	urldate = {2025-06-03},
}

@online{noauthor_image_nodate,
	title = {image segmentation evaluation metrics - Recherche Google},
	url = {https://www.google.com/search?sca_esv=6fb1b3dcd7ad6b24&sxsrf=AHTn8zoxNZwWgJauihIZEY2bWg0J7qsK0g:1747119639633&q=image+segmentation+evaluation+metrics&udm=2&fbs=ABzOT_BnMAgCWdhr5zilP5f1cnRvJ3SHQcDVxkdpDyHwlRhdNfno-ClRh0PKqyvFYyTkfIeCoWIR5C1jUN_wgcDJ8Cdwgf887lFxRCsZbfrfzJNeHuzDoUCV8vUanlRiovNa88HW2-kTk0lb9AI5WwawUUKXj7H7YbDT_0pk09eBv8DMBg0L3nJA6Fq2iRtrdalgnPX2Y5_E&sa=X&ved=2ahUKEwiT-__r75-NAxVCTaQEHfXvICMQtKgLegQIEBAB&biw=1912&bih=954&dpr=1},
	urldate = {2025-06-03},
}

@software{hope1337_hope1337yowov3_2025,
	title = {Hope1337/{YOWOv}3},
	url = {https://github.com/Hope1337/YOWOv3},
	author = {Hope1337},
	urldate = {2025-06-03},
	date = {2025-06-02},
	note = {original-date: 2024-04-17T11:25:33Z},
}

@online{noauthor_yowov3_nodate,
	title = {{YOWOv}3: An Efficient and Generalized Framework for Human Action Detection and Recognition},
	url = {https://arxiv.org/html/2408.02623v1},
	urldate = {2025-06-03},
}

@software{yang_yjh0410yowov2_2025,
	title = {yjh0410/{YOWOv}2},
	rights = {{MIT}},
	url = {https://github.com/yjh0410/YOWOv2},
	abstract = {The second generation of {YOWO} action detector.},
	author = {Yang, Jianhua},
	urldate = {2025-06-03},
	date = {2025-05-28},
	note = {original-date: 2023-02-06T02:09:48Z},
	keywords = {action-detection, one-stage-detector, you-only-watch-once},
}

@software{mattnudi_mattnudibee-detection_2025,
	title = {mattnudi/bee-detection},
	url = {https://github.com/mattnudi/bee-detection},
	abstract = {Repository for sharing of pre-trained yolov5 weights for honey bee detection},
	author = {mattnudi},
	urldate = {2025-06-03},
	date = {2025-05-11},
	note = {original-date: 2022-03-23T16:26:37Z},
}

@software{jocher_yolov5_2020,
	title = {{YOLOv}5 by Ultralytics},
	rights = {{GPL}-3.0},
	url = {https://github.com/ultralytics/yolov5},
	abstract = {bee detection {ML} model (based on {YOLOv}5)},
	version = {7.0},
	author = {Jocher, Glenn},
	urldate = {2025-06-03},
	date = {2020-05},
	doi = {10.5281/zenodo.3908559},
}

@online{phd_simple_2024,
	title = {Simple {YOLO} custom dataset automatic generation},
	url = {https://medium.com/@telega.slawomir.ai/simple-yolo-custom-dataset-automatic-generation-295f5c9ccfb2},
	abstract = {Simple method of automatic dataset generation for object detection},
	titleaddon = {Medium},
	author = {{PhD}, Slawomir Telega},
	urldate = {2025-06-03},
	date = {2024-08-16},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/B2TBFAIF/simple-yolo-custom-dataset-automatic-generation-295f5c9ccfb2.html:text/html},
}

@online{laudari_yolov10_2024,
	title = {{YOLOv}10: A Step-by-Step Guide to Object Detection on a Custom Dataset},
	url = {https://medium.com/@sudiplaudari/yolov10-a-step-by-step-guide-to-object-detection-on-a-custom-dataset-9f3e3e56921c},
	shorttitle = {{YOLOv}10},
	abstract = {Overview},
	titleaddon = {Medium},
	author = {Laudari, Sudip},
	urldate = {2025-06-03},
	date = {2024-07-11},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/F8QRR6TD/yolov10-a-step-by-step-guide-to-object-detection-on-a-custom-dataset-9f3e3e56921c.html:text/html},
}

@online{noauthor_google_nodate,
	title = {Google Colab},
	url = {https://colab.research.google.com/drive/1qK7Z6nQbLGot3ySQdye29_VnfgyTEaN7#scrollTo=647bc45b5f989751},
	urldate = {2025-06-03},
	langid = {french},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/X5RCDNRT/1qK7Z6nQbLGot3ySQdye29_VnfgyTEaN7.html:text/html},
}

@online{noauthor_httpsarxivorgpdf240802623_nodate,
	title = {https://arxiv.org/pdf/2408.02623},
	url = {https://arxiv.org/pdf/2408.02623},
	urldate = {2025-06-06},
	file = {https\://arxiv.org/pdf/2408.02623:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/3RP383JH/2408.pdf:application/pdf},
}

@misc{dang_yowov3_2024,
	title = {{YOWOv}3: An Efficient and Generalized Framework for Human Action Detection and Recognition},
	url = {http://arxiv.org/abs/2408.02623},
	doi = {10.48550/arXiv.2408.02623},
	shorttitle = {{YOWOv}3},
	abstract = {Our new framework, {YOWOv}3, is an enhanced version of {YOWOv}2 that we provide in this study with a focus on Spatio Temporal Action Detection task. This framework is made by offering a more accessible approach to experiment deeply with different configurations and to easily customize different model components, which minimizes the amount of labor needed to comprehend and alter the source code. {YOWOv}3 outperforms {YOWOv}2 on two popular datasets ({UCF}101-24 and {AVAv}2.2) for Human Action Detection and Recognition. In particular, the prior model, {YOWOv}2, with 109.7M parameters and 53.6 {GFLOPs}, obtains a {mAP} of 85.2\% and 20.3\% on {UCF}101-24 and {AVAv}2.2, respectively. On the other hand, our model, {YOWOv}3, obtains a {mAP} of 20.31\% on {AVAv}2.2 and 88.33\% on {UCF}101-24, by utilizing just 39.8 {GFLOPs} and 59.8M parameters. The outcomes show that {YOWOv}3 achieves equivalent performance with a significant reduction in the number of parameters and {GFLOPs}.},
	number = {{arXiv}:2408.02623},
	publisher = {{arXiv}},
	author = {Dang, Duc Manh Nguyen and Duong, Viet Hang and Wang, Jia Ching and Duc, Nhan Bui},
	urldate = {2025-06-06},
	date = {2024-08-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2408.02623 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/4XVESIE6/Dang et al. - 2024 - YOWOv3 An Efficient and Generalized Framework for Human Action Detection and Recognition.pdf:application/pdf},
}

@software{yang_yjh0410ava_dataset_2025,
	title = {yjh0410/{AVA}\_Dataset},
	url = {https://github.com/yjh0410/AVA_Dataset},
	abstract = {download {AVA} dataset},
	author = {Yang, Jianhua},
	urldate = {2025-06-06},
	date = {2025-06-03},
	note = {original-date: 2022-08-15T03:13:03Z},
}

@software{hope1337_hope1337yowov3_2025-1,
	title = {Hope1337/{YOWOv}3},
	url = {https://github.com/Hope1337/YOWOv3},
	author = {Hope1337},
	urldate = {2025-06-06},
	date = {2025-06-02},
	note = {original-date: 2024-04-17T11:25:33Z},
}

@online{noauthor_clahe_nodate,
	title = {{CLAHE} Histogram Equalization - {OpenCV}},
	url = {https://www.geeksforgeeks.org/clahe-histogram-eqalization-opencv/},
	abstract = {Your All-in-One Learning Portal: {GeeksforGeeks} is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
	titleaddon = {{GeeksforGeeks}},
	urldate = {2025-06-17},
	langid = {american},
	note = {Section: Python},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/ATLJRVL2/clahe-histogram-eqalization-opencv.html:text/html},
}

@online{noauthor_httpswwwignfrpublications--l-igninstitutcarte_identitepresentation_ignpdf_nodate,
	title = {https://www.ign.fr/publications-de-l-ign/institut/carte\_identite/presentation\_ign.pdf},
	url = {https://www.ign.fr/publications-de-l-ign/institut/carte_identite/presentation_ign.pdf},
	urldate = {2025-06-27},
	file = {https\://www.ign.fr/publications-de-l-ign/institut/carte_identite/presentation_ign.pdf:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/QDI8L9H6/presentation_ign.pdf:application/pdf},
}

@online{noauthor_acteur_2023,
	title = {Acteur de référence de la donnée géolocalisée souveraine, l'{IGN} avance sur un chemin à baliser},
	url = {https://www.senat.fr/rap/r22-017/r22-017.html},
	titleaddon = {Sénat},
	urldate = {2025-06-27},
	date = {2023-04-07},
	langid = {french},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/ECMXVYXD/r22-0172.html:text/html},
}

@online{noauthor_terre_2019,
	title = {Terre, climat : qu’est-ce que l’Anthropocène, ère géologique {\textbar} vie-publique.fr},
	url = {https://www.vie-publique.fr/parole-dexpert/271086-terre-climat-quest-ce-que-lanthropocene-ere-geologique},
	shorttitle = {Terre, climat},
	abstract = {L’Anthropocène est une nouvelle époque géologique qui se caractérise par l’avènement des hommes comme principale force de changement sur Terre, surpassant les forces géophysiques. C’est l’âge des humains ! Celui d’un désordre planétaire inédit. {\textbar}},
	urldate = {2025-06-27},
	date = {2019-10-08},
	langid = {french},
}

@online{noauthor_lastig_nodate,
	title = {{LASTIG}},
	url = {https://www.umr-lastig.fr/},
	urldate = {2025-06-29},
	file = {LASTIG:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/9B7J9ZL4/www.umr-lastig.fr.html:text/html},
}

@inreference{noauthor_geomatique_2025,
	title = {Géomatique},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://fr.wikipedia.org/w/index.php?title=G%C3%A9omatique&oldid=223284952},
	abstract = {La géomatique (mot issu de la contraction des termes « géographie » et « informatique ») regroupe l'ensemble des outils et méthodes permettant d'acquérir, de représenter, d'analyser et d'intégrer des données géographiques. La géomatique consiste donc en au moins trois activités distinctes : collecte, traitement et diffusion des données géographiques.
La géomatique est étroitement liée à l'information géographique, qui est la représentation d'un objet ou d'un phénomène localisé dans l'espace. Le domaine de la géomatique englobe les {SIG} et les dépasse.
Les professionnels travaillant à l'étude de la géomatique et de ses logiciels sont appelés des géomaticiens (chef de projet ou ingénieur en système d'information géographique, administrateur {SIG}, technicien cartographe ou en traitement des données, gestionnaire de bases de données spatiales, architecte {SIG}…).},
	booktitle = {Wikipédia},
	urldate = {2025-07-01},
	date = {2025-02-23},
	langid = {french},
	note = {Page Version {ID}: 223284952},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/B2NB7P9S/Géomatique.html:text/html},
}

@online{jarry__2022,
	title = {« La carte ne cherche pas à concurrencer le {GPS}, elle occupe toute sa place à côté des dispositifs de guidage »},
	url = {https://www.lemag-ic.fr/case-studies/la-carte-ne-cherche-pas-a-concurrencer-le-gps-elle-occupe-toute-sa-place-a-cote-des-dispositifs-de-guidage/},
	abstract = {Si le {GPS} fait désormais partie de notre quotidien, dans nos voitures, sur nos téléphones, il n’a pas fait disparaître pour autant la carte routière qui reste le point de départ de nombreux voyages. Leader en France avec 80\% de parts de marché, les Éditions Michelin l’ont bien compris, avec une offre qui s’adapte aux nouvelles attentes et aux nouveaux moyens de déplacements comme le vélo ou le van. Chargée de communication et marketing opérationnel aux Éditions Michelin, Julie Duhourcau nous montre la voie.},
	titleaddon = {{IC}},
	author = {{JARRY}, Cécile},
	urldate = {2025-07-01},
	date = {2022-05-30},
	langid = {french},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/ST9FK7MH/la-carte-ne-cherche-pas-a-concurrencer-le-gps-elle-occupe-toute-sa-place-a-cote-des-dispositifs.html:text/html},
}

@article{noauthor_decision_nodate,
	title = {Décision n° 06-D-23 du 21 juillet 2006 relative à la situation de la concurrence dans les secteurs de l'édition cartographique et de l'information touristique},
	langid = {french},
	file = {PDF:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/JRCCP5IG/Décision n° 06-D-23 du 21 juillet 2006 relative à la situation de la concurrence dans les secteurs d.pdf:application/pdf},
}

@online{keldenich_recall_2021,
	title = {Recall, Precision, F1 Score - Explication Simple Métrique en {ML}},
	url = {https://inside-machinelearning.com/recall-precision-f1-score/},
	abstract = {Le Recall, la Precision, le F1 Score comment retenir facilement leur utilité et ce que ces métriques impliquent ?},
	author = {Keldenich, Tom},
	urldate = {2025-07-08},
	date = {2021-09-02},
	langid = {french},
	note = {Section: Les Bases},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/KGE5CIYK/recall-precision-f1-score.html:text/html},
}

@online{ultralytics___init___nodate,
	title = {\_\_init\_\_},
	url = {https://docs.ultralytics.com/reference/utils/__init__},
	abstract = {Explore the comprehensive reference for ultralytics.utils in the Ultralytics library. Enhance your {ML} workflow with these utility functions.},
	author = {Ultralytics},
	urldate = {2025-07-08},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/NLR7JTVT/__init__.html:text/html},
}

@online{noauthor_opencv_nodate-1,
	title = {{OpenCV}: Getting Started with Videos},
	url = {https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html},
	urldate = {2025-07-22},
	file = {OpenCV\: Getting Started with Videos:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/EYD8T3RY/tutorial_py_video_display.html:text/html},
}

@software{duwal_iamrukeshduwalyolov11_real_time_object_detection_with_deepsort_2025,
	title = {iamrukeshduwal/yolov11\_real\_time\_object\_detection\_with\_DeepSORT},
	url = {https://github.com/iamrukeshduwal/yolov11_real_time_object_detection_with_DeepSORT},
	author = {Duwal, Rukesh},
	urldate = {2025-07-25},
	date = {2025-06-08},
	note = {original-date: 2024-10-08T05:07:14Z},
}

@online{noauthor_opencv_nodate-2,
	title = {{OpenCV}: Drawing Functions in {OpenCV}},
	url = {https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html},
	urldate = {2025-07-28},
	file = {OpenCV\: Drawing Functions in OpenCV:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/I9JIKTH3/tutorial_py_drawing_functions.html:text/html},
}

@online{noauthor_yolo-bytetrack-vehicle-trackingobject_trackingpy_nodate,
	title = {yolo-bytetrack-vehicle-tracking/object\_tracking.py at main · {VuBacktracking}/yolo-bytetrack-vehicle-tracking},
	url = {https://github.com/VuBacktracking/yolo-bytetrack-vehicle-tracking/blob/main/object_tracking.py},
	urldate = {2025-07-28},
	file = {yolo-bytetrack-vehicle-tracking/object_tracking.py at main · VuBacktracking/yolo-bytetrack-vehicle-tracking:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/2W8AYQ4U/object_tracking.html:text/html},
}

@software{wojke_nwojkedeep_sort_2025,
	title = {nwojke/deep\_sort},
	rights = {{GPL}-3.0},
	url = {https://github.com/nwojke/deep_sort},
	abstract = {Simple Online Realtime Tracking with a Deep Association Metric},
	author = {Wojke, Nicolai},
	urldate = {2025-07-29},
	date = {2025-07-29},
	note = {original-date: 2017-02-04T13:17:42Z},
}

@online{noauthor_yolo-deepsort-trackingmainpy_nodate,
	title = {{YOLO}-{DeepSort}-Tracking/main.py at main · seonghwan97/{YOLO}-{DeepSort}-Tracking},
	url = {https://github.com/seonghwan97/YOLO-DeepSort-Tracking/blob/main/main.py},
	abstract = {{YOLOv}11 + {DeepSort} Human Tracking. Contribute to seonghwan97/{YOLO}-{DeepSort}-Tracking development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2025-07-29},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/FEVVTXES/main.html:text/html},
}

@online{noauthor_yolo-bytetrack-vehicle-trackingobject_trackingpy_nodate-1,
	title = {yolo-bytetrack-vehicle-tracking/object\_tracking.py at main · {VuBacktracking}/yolo-bytetrack-vehicle-tracking},
	url = {https://github.com/VuBacktracking/yolo-bytetrack-vehicle-tracking/blob/main/object_tracking.py},
	abstract = {Vehicle Tracking and Counting using Yolo and {ByteTrack} - {VuBacktracking}/yolo-bytetrack-vehicle-tracking},
	titleaddon = {{GitHub}},
	urldate = {2025-07-29},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/SFAF26KV/object_tracking.html:text/html},
}

@online{noauthor_bytetrackexpsexamplemot_nodate,
	title = {{ByteTrack}/exps/example/mot at main · {FoundationVision}/{ByteTrack}},
	url = {https://github.com/FoundationVision/ByteTrack/tree/main/exps/example/mot},
	abstract = {[{ECCV} 2022] {ByteTrack}: Multi-Object Tracking by Associating Every Detection Box - {FoundationVision}/{ByteTrack}},
	titleaddon = {{GitHub}},
	urldate = {2025-07-29},
	langid = {english},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/VMTH59TC/mot.html:text/html},
}

@online{noauthor_enhance_nodate,
	title = {Enhance Local Contrast ({CLAHE})},
	url = {https://imagej.github.io/plugins/clahe},
	abstract = {The {ImageJ} wiki is a community-edited knowledge base on topics relating to {ImageJ}, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including {ImageJ}2, Fiji, and others.},
	titleaddon = {{ImageJ} Wiki},
	urldate = {2025-08-08},
	note = {Section: Filtering},
	file = {Snapshot:/home/ecarrasco/snap/zotero-snap/common/Zotero/storage/89W6KI6U/clahe.html:text/html},
}
